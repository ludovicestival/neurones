# Rapport - Ludovic Estival

# TP 1 - Perceptron

## Partie 1 - Perceptron simple

### Exercice 1

Mettre les images

### Exercice 2

1. Pourquoi `heaviside` pose un problème pour l'apprentissage par gradient ?

L'apprentissage par gradient nécessite l'utilisation des dérivées or `heaviside` n'est pas dérivable. On est donc obligé de passer par la distribution de Dirac.

2. Dans quels cas utiliser `sigmoid` ou `tahn` ?

3. Pourquoi `relu` est-elle si populaire dans les réseaux profonds ?

4. Quel est l'avantage de `leaky relu` ?

## Partie 2 - Apprentissage du perceptron

### Exercice 3

1. Que se passe t-il si `n` est trop grand ?

2. Et s'il est trop petit ?

3. Existe t-il une valeur idéale de `n` ?

4. Peut-on faire varier `n` au cours du temps ?

5. Quelle stratégie pouvez vous imaginer ?


# TP 2 - Perceptron multi-couches

